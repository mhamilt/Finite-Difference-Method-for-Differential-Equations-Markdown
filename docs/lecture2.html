<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Lecture 2</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    iframe{
        margin: 1em 0;
        width: 35em;
        width: 95%;
        height: 500px;
        max-width: 36em;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Lecture 2</h1>
</header>
<h1 id="chap:SHO">The Simple Harmonic Oscillator</h1>
<p>Simple harmonic motion is obtained under linear condition, that is <span class="math display">\[\phi = \frac{Kx^2}{2}.\]</span> Under such choice, <a href="#eq:SHO" data-reference-type="eqref" data-reference="eq:SHO">[eq:SHO]</a> becomes <span class="math display">\[\label{eq:SHOreal}
    \frac{d^2 x}{dt^2} = - \omega_0^2 x,\]</span> where the radian natural frequency <span class="math inline">\(\omega_0 = \sqrt{K/m}\)</span> was introduced. There are multiple reasons to be wanting to study the simple harmonic oscillator here: a variety of mechanical systems can be approximated by <a href="#eq:SHOreal" data-reference-type="eqref" data-reference="eq:SHOreal">[eq:SHOreal]</a>, see e.g. Fig. <a href="#fig:oscExamples" data-reference-type="ref" data-reference="fig:oscExamples">1.1</a>. Furthermore, as will be seen in later lectures, distributed systems may themselves be approximated as a bank of parallel oscillators, each one corresponding to one “mode” of vibration. Finally, the simple harmonic oscillator possesses exact (i.e. closed-form) solutions, both analytically and numerically, and is thus a very useful test case for the numerical schemes that will be introduced below. It was seen that the solution is expressed as the sum of two periodic functions, i.e. <span class="math display">\[x(t) = a \cos(\omega_0 t) + b \sin{\omega_0 t}.\]</span> The constants <span class="math inline">\(a,b\)</span> are uniquely determined from the intial conditions. Setting <span class="math inline">\(x(t=0)=x_0\)</span> and <span class="math inline">\(\frac{dx}{dt}(x=0)=v_0\)</span>, one gets <span class="math display">\[\label{eq:SHOexact}
    x(t) = x_0 \cos(\omega_0 t) + \frac{v_0}{\omega_0}\sin(\omega_0 t)\]</span> The energy components are obtained explictly as <span class="math display">\[E_k = \frac{m}{2}\left(x_0\omega_0\sin(\omega_0 t) - v_0 \cos(\omega_0 t) \right)^2, \quad E_p = \frac{K}{2}\left( x_0 \cos(\omega_0 t) + \frac{v_0}{\omega_0}\sin(\omega_0 t) \right)^2.\]</span> Summing the two expressions together, one gets <span class="math display">\[H(t) = \frac{m v_0^2}{2} + \frac{K x_0^2}{2} = H_0,\]</span> i.e. <a href="#eq:EnCons" data-reference-type="eqref" data-reference="eq:EnCons">[eq:EnCons]</a>.</p>
<figure>
<embed src="Figures/OscillatorsExamples.pdf" id="fig:oscExamples" /><figcaption aria-hidden="true">Examples of harmonic oscillators. The mass-spring system (a), and the series RLC circuit (b) are usually given as examples of harmonic oscillators. The case of a cantilever beam with a point mass (c), and the pendulum (d) may be approximated as harmonic oscillators in the case of small vibrations.</figcaption>
</figure>
<h2 id="a-finite-difference-scheme">A finite difference scheme</h2>
<p>Consider the time series <span class="math inline">\(x^n\)</span>, approximating the true solution <span class="math inline">\(x(t)\)</span> of <a href="#eq:SHOreal" data-reference-type="eqref" data-reference="eq:SHOreal">[eq:SHOreal]</a>. As a first example of a working finite difference scheme, consider <span class="math display">\[\label{eq:Scheme1}
    \delta_{tt}x^n = -\omega_0^2 x^n.\]</span> Expanding out the operator, one gets <span class="math display">\[\label{eq:FDbasic}
    x^{n+1} = x^n(2-\omega_0^2 k^2) - x^{n-1},\]</span> and hence the update requires one multiply and one sum. Clearly, it is convenient to store the value <span class="math inline">\(2-\omega_0^2 k^2\)</span> offline, so that one need not recompute this value at each time step. The update <a href="#eq:FDbasic" data-reference-type="eqref" data-reference="eq:FDbasic">[eq:FDbasic]</a> is <em>explicit</em>: by this term, we denote a scheme such that <span class="math display">\[x^{n+1} = f(x^n,x^{n-1}), \qquad \text{(explicit scheme)}\]</span> We will soon encounter schemes that are instead <em>implicit</em>, i.e. <span class="math display">\[\label{eq:ImplSchemeDef}
    g(x^{n+1})= f(x^n,x^{n-1}), \qquad \text{(implicit scheme)}\]</span> where <span class="math inline">\(g\)</span> is generally a nonlinear function in <span class="math inline">\(x^{n+1}\)</span>.</p>
<p>Though scheme <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a> looks reasonable, there is no guarantee (for the moment) that the computed solutions are indeed an approximate form of the true solution <span class="math inline">\(x(t)\)</span>. In some cases, as will be seen shortly, the time series computed by <span class="math inline">\(\eqref{eq:Scheme1}\)</span> diverges, in some other cases, it remains bounded. The next few sections will explain the idea of <em>convergence</em>, and the closely linked idea of <em>stability</em>. A notion of stability may be formalised as follows. We say that a scheme is stable in stability region <span class="math inline">\(\Lambda \subseteq \mathbb{R}^+\)</span> if, for any positive time constant <span class="math inline">\(\tau \leq n k\)</span>, there exist a positive index <span class="math inline">\(M\)</span> such that <span class="math display">\[\label{eq:StabDef}
    |x^n| \leq C_\tau \sum_{m = 0}^M |x^m|\]</span> for a constant <span class="math inline">\(C_\tau &gt; 0\)</span> independent of <span class="math inline">\(k \in \Lambda\)</span>. In practice, stability is defined as a bound on the time series including the first <span class="math inline">\(M+1\)</span> steps.</p>
<h3 id="sec:FreqDomSHO">Stability via frequency domain analysis</h3>
<p>As anticipated earlier, frequency-domain techinques may be employed in the analysis of stability of linear, time-invariant discrete systems, such as <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a>. For that, an <em>ansatz</em> of the form <a href="#eq:ansatz" data-reference-type="eqref" data-reference="eq:ansatz">[eq:ansatz]</a> is substituted: <span class="math display">\[x^n = \hat x z^{n}.\]</span> In this equation, notation is a little mixed-up, since on the left-hand side <span class="math inline">\(n\)</span> denotes the time index, whereas on the right-hand side it is an exponent! In practice, we keep the same apex notation in both cases, for the sake of notation, but the meaning is very different. We get <span class="math display">\[\hat x z^n\left( z - (2-\omega_0 k^2) + z^{-1}\right) = 0, \quad \rightarrow \quad z_{\pm} = \frac{2 - \omega_0^2 k^2 \pm \omega_0^2 k^2\sqrt{1 - \frac{4}{\omega_0^2 k^2}}}{2}.\]</span> Hence, the solution is <span class="math display">\[\label{eq:sol_z_SHO}
    x^n =  A_+ z^n_+ + A_- z^n_-,\]</span> for complex constants <span class="math inline">\(A_\pm\)</span>. We assume that the scheme is started using two starting values <span class="math inline">\(x^0, x^1\)</span> (obtained from <span class="math inline">\(x_0\)</span>, <span class="math inline">\(v_0\)</span> of the continuous problem). Then <span class="math display">\[x^0 = A_+ + A_-, \quad x^1 = A_+ z_+ + A_- z_-.\]</span> From these, the complex constants are obtained as <span class="math display">\[\label{eq:ApAm}
    A_+ = \frac{x^0z_- - x^1}{z_- - z_+},\,\,\, A_- = \frac{x^1 - x^0z_+ }{z_- - z_+}.\]</span> If the square root in <span class="math inline">\(z_\pm\)</span> is a real number, than <span class="math inline">\(z_-\)</span> has magnitude larger than unity, and the solution <span class="math inline">\(x^n\)</span> will therefore grow exponentially over time: this is <em>instability</em>. On the other hand, when the square root in imaginary, then <span class="math inline">\(z_\pm\)</span> become oscillating, and <span class="math inline">\(z_\pm\)</span> are complex conjugates. This condition is obtained as <span class="math display">\[\label{eq:StabCondSHO}
    k &lt; \frac{2}{\omega_0},\]</span> which is an upper bound on the time step, once the natural frequency of the oscillator is set. In the case of oscillating solutions, one has <span class="math display">\[z_\pm = r e^{\pm j \theta},\]</span> with <span class="math inline">\(r = 1\)</span>, and <span class="math inline">\(\tan\theta = \left(\omega_0^2k^2\sqrt{\frac{4}{\omega_0^2 k^2}-1}\right)/(2-\omega_0^2 k^2)\)</span>.</p>
<p>To check stability, definition <a href="#eq:StabDef" data-reference-type="eqref" data-reference="eq:StabDef">[eq:StabDef]</a> is applied, to give <span class="math display">\[\label{eq:SHObound} 
    |x^n| = |A_+ r^n e^{j \theta n} + A_- r^n e^{-j \theta n}| \leq |A_+| + |A_-| \leq \frac{|x^0|+|x^1|}{|\sin \theta|} \triangleq C_\tau \sum_{m=0}^1 |x^m|,\]</span> and thus the absolute value of the solution at the time <span class="math inline">\(n&gt;1\)</span> is bounded in terms of the values at <span class="math inline">\(n=0,1\)</span>, with bounding constant <span class="math inline">\(C_\tau = 1/|\sin\theta|\)</span>. (The first inequality in the above was obtained via the triangle inequality. Then, the fact that <span class="math inline">\(|r|=|e^{\pm j\theta}|=1\)</span> was used, and finally the values from <a href="#eq:ApAm" data-reference-type="eqref" data-reference="eq:ApAm">[eq:ApAm]</a> were substituted in). A numerical check of the current bound is given in Fig. <a href="#fig:SHObounds" data-reference-type="ref" data-reference="fig:SHObounds">1.2</a>.</p>
<figure>
<img src="Figures/boundsSHO.png" id="fig:SHObounds" alt="Simple Harmonic Oscillator. Numerical check on bound [eq:SHObound]. The sample rate used in the examples is f_s = 2000 Hz. Starting values are x^0 = x^1 = 1. Dashed line line is bound [eq:SHObound], characteristic frequency as indicated, with \omega_{max}=2/k." /><figcaption aria-hidden="true">Simple Harmonic Oscillator. Numerical check on bound <a href="#eq:SHObound" data-reference-type="eqref" data-reference="eq:SHObound">[eq:SHObound]</a>. The sample rate used in the examples is <span class="math inline">\(f_s = 2000\)</span> Hz. Starting values are <span class="math inline">\(x^0 = x^1 = 1\)</span>. Dashed line line is bound <a href="#eq:SHObound" data-reference-type="eqref" data-reference="eq:SHObound">[eq:SHObound]</a>, characteristic frequency as indicated, with <span class="math inline">\(\omega_{max}=2/k\)</span>.</figcaption>
</figure>
<p>Note that the same condition may be arrived at via <em>von Neumann</em> analysis. Recall the DTFT of the <span class="math inline">\(\delta_{tt}\)</span> operator, as per <a href="#eq:DTFTdtt" data-reference-type="eqref" data-reference="eq:DTFTdtt">[eq:DTFTdtt]</a>. Transforming <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a> in the frequency domain accordingly, one gets <span class="math display">\[\left(-\frac{4}{k^2}\sin^2\left( \frac{\omega k}{2}\right) + \omega_0^2\right)\mathcal{X}\left\{ x^n\right\} = 0.\]</span> One must impose <span class="math inline">\(0 \leq \sin^2\left( \frac{\omega k}{2}\right) \leq 1\)</span>, which is possible if and only if <a href="#eq:StabCondSHO" data-reference-type="eqref" data-reference="eq:StabCondSHO">[eq:StabCondSHO]</a> holds. If this condition is violated, one has that the frequency <span class="math inline">\(\omega\)</span> becomes pure imaginary, thus the sine becomes a hyerbolic sine, with unbounded growth (instability).</p>
<h4 id="stability-via-energy-analysis">Stability via energy analysis</h4>
<p>The discussion of Sec. <a href="#sec:EnAnGen" data-reference-type="ref" data-reference="sec:EnAnGen">[sec:EnAnGen]</a> suggests that, if the model problem can be shown to have a conserved total energy, with non-negative kinetic and potential terms, then the solution can be bounded in terms of the energy. It may be tempting to try to find a discrete version of <a href="#eq:EnAnGen" data-reference-type="eqref" data-reference="eq:EnAnGen">[eq:EnAnGen]</a>, valid in the discrete case. To that end, <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a> is multiplied by <span class="math inline">\(m \delta_{t\cdot}x^n\)</span>, to get <span class="math display">\[\label{eq:SHOen1}
    m \delta_{t\cdot}x^n (\delta_{tt}x^n +\omega_0^2 x^n) = 0\]</span> A couple of useful identities (that will be used throughout) are given here: <span class="math display">\[\label{eq:IdsFD}
    \delta_{t\cdot}x^n \, \delta_{tt}x^n = \delta_{t+}\left( \frac{(\delta_{t-}x^n)^2}{2}\right), \,\, \delta_{t\cdot}x^n \, x^n = \delta_{t+}\left( \frac{x^n e_{t-}x^n}{2}\right).\]</span> These are proven by simple algebra. Using these identities in <a href="#eq:SHOen1" data-reference-type="eqref" data-reference="eq:SHOen1">[eq:SHOen1]</a>, one gets <span class="math display">\[\delta_{t+}\left( \frac{m}{2}{(\delta_{t-}x^n)^2} + \frac{K}{2} {x^n e_{t-}x^n} \right) = 0,\]</span> that is a discrete counterpart of <a href="#eq:En2" data-reference-type="eqref" data-reference="eq:En2">[eq:En2]</a>. Multiplication by <span class="math inline">\(m\)</span> was here used so to yield units of energy in the expression within the brakets, though of course one may obtain conserved energy per unit mass via multiplication by <span class="math inline">\(\delta_{t\cdot}x^n\)</span> alone. It is easy, in the above, to recognise a discrete approximation to the continuous energy balance <a href="#eq:En1" data-reference-type="eqref" data-reference="eq:En1">[eq:En1]</a>. In this case, one may define an <em>interleaved</em> time series <span class="math inline">\(\mathfrak{h}^{n-1/2}\)</span>, corresponding to the discrete conserved energy: <span class="math display">\[\label{eq:DiscEnSHO}
    \mathfrak{h}^{n-1/2} \triangleq \frac{m}{2}{(\delta_{t-}x^n)^2} + \frac{K}{2} {x^n e_{t-}x^n} = \mathfrak{h}^{1/2}.\]</span> In light of the discussion in the continuous case, one may of course use energy conservation as a means to bound the growth of solutions over time. The problem here, is that <span class="math inline">\(\mathfrak{h}^{n-1/2}\)</span> may <em>not</em> be positive, since the potential energy is of indefinite sign. Instances leading to negative energy overall are a manifestation of instability, and must be avoided. It may be useful, then, to bound the potential term in the energy expression. Using <span class="math display">\[x^n e_{t-}x^n  =  (\mu_{t-}x^n)^2 - \frac{k^2}{4}\left(\delta_{t-}x^n \right)^2,\]</span> the total energy is <span class="math display">\[\label{eq:ModEnSHO}
    \mathfrak{h}^{n-1/2} = \frac{m\left(\delta_{t-}x^n \right)^2}{2} \left(1 - \frac{\omega_0^2 k^2}{4}\right) + \frac{K (\mu_{t+}x^n)^2}{2} \geq \frac{m\left(\delta_{t-}x^n \right)^2}{2} \left(1 - \frac{\omega_0^2 k^2}{4}\right),\]</span> and thus the total energy will be non-negative if and only if <a href="#eq:StabCondSHO" data-reference-type="eqref" data-reference="eq:StabCondSHO">[eq:StabCondSHO]</a> is satisfied. Fig. <a href="#fig:EnConsSHO" data-reference-type="ref" data-reference="fig:EnConsSHO">1.3</a> shows the energy components and error for scheme <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a>. The energy components are given as per <a href="#eq:DiscEnSHO" data-reference-type="eqref" data-reference="eq:DiscEnSHO">[eq:DiscEnSHO]</a>, and it is remarked that the potential term <em>does</em> become negative at times. It is the <em>overall</em> energy that is positive. Of course, the equivalent expression in <a href="#eq:ModEnSHO" data-reference-type="eqref" data-reference="eq:ModEnSHO">[eq:ModEnSHO]</a> has modified expressions for the kinetic and potential energies, that are always non-negative under stability condition <a href="#eq:StabCondSHO" data-reference-type="eqref" data-reference="eq:StabCondSHO">[eq:StabCondSHO]</a>.</p>
<figure>
<img src="Figures/EnergyErr.png" id="fig:EnConsSHO" alt="Energy behaviour of scheme [eq:Scheme1]. Left: energy components. Kinetic (dashed), potential (dash-dotted), total (solid). Right: energy error \mathfrak{h}^{n-1/2}/\mathfrak{h}^{1/2}-1. The oscillator is initialised with x_0=v_0=1, and \omega_0=100 rad/s." /><figcaption aria-hidden="true">Energy behaviour of scheme <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a>. Left: energy components. Kinetic (dashed), potential (dash-dotted), total (solid). Right: energy error <span class="math inline">\(\mathfrak{h}^{n-1/2}/\mathfrak{h}^{1/2}-1\)</span>. The oscillator is initialised with <span class="math inline">\(x_0=v_0=1\)</span>, and <span class="math inline">\(\omega_0=100\)</span> rad/s.</figcaption>
</figure>
<h4 id="consistency-and-accuracy">Consistency and accuracy</h4>
<p>We are now introducing the idea of <em>local truncation error</em> (LTE), denoted here <span class="math inline">\(\varepsilon^n\)</span>. Applying the finite difference scheme to the true solution <span class="math inline">\(x(t)\)</span> yields a definiton of the LTE as <span class="math display">\[\label{eq:LTEdef}
    \delta_{tt}x(t_n) + \omega_0^2 x(t_n) = \varepsilon^n.\]</span> Using Taylor series arguments, as per <a href="#eq:Errs" data-reference-type="eqref" data-reference="eq:Errs">[eq:Errs]</a>, one gets <span class="math display">\[\left( \frac{d^2 x(t)}{dt} + \omega_0^2 x(t)\right)|_{t=t_n} + O(k^2) = \varepsilon^n,\]</span> and since <span class="math inline">\(x(t)\)</span> is the true solution, one recovers <span class="math inline">\(\varepsilon^n = O(k^2)\)</span>. The behaviour of the LTE as a function of <span class="math inline">\(k\)</span> describes the idea of <em>consistency</em>: a scheme is said to be consistent if <span class="math display">\[\lim_{k\rightarrow 0}\varepsilon^n = 0.\]</span> In practice, consistent schemes are such that the local error becomes small as <span class="math inline">\(k\)</span> is decreased. Usually, <span class="math inline">\(\varepsilon = O(k^p)\)</span>, and one may conclude that the scheme is <span class="math inline">\(p^{th}\)</span>-order accurate. Of course, this is not entirely true, since the question of accuracy is tightly bound to the ideas of stability and convergence: higher-accurate schemes may <em>never</em> converge for a given model problem. The idea of accuracy is only going to be meaningful when a scheme is provably stable in some manner. As an example, consider a fourth-order accurate difference operator discretising the second time derivative: <span class="math display">\[\label{eq:FourthOrderdtt}
    \bar\delta_{tt} x(t) = \left(\frac{-e_{t+}^2 + 16 e_{t+}- 30 + 16e_{t+}- e_{t+}^2}{12k^2}\right) x(t) = \frac{d^2 x}{dt^2} + O(k^4).\]</span> Though technically “higher” accurate, this approximation is always unstable, even for the simple problem of a free particle (<span class="math inline">\(\phi = 0\)</span> in <a href="#eq:PhiF" data-reference-type="eqref" data-reference="eq:PhiF">[eq:PhiF]</a>). Using the test solution <span class="math inline">\(x^n = \hat x z^n\)</span> for this test case, one has <span class="math display">\[\hat x z^n \left(-z^2 + 16 z - 30 + 16 z^{-1} - z^{-2}\right) = 0,\]</span> and it is easy to verify that there exist one (real) root <span class="math inline">\(z \approx 13.9282\)</span> for which clearly <span class="math inline">\(|z|&gt;1\)</span>. The scheme is unstable, and the higher accuracy of error of <span class="math inline">\(\bar\delta_{tt}\)</span> has no real advantage.</p>
<h4 id="convergence">Convergence</h4>
<p>In turn, what we are really interested in is the evolution of the global error <span class="math inline">\(E^n\)</span>, which must remain bounded. For stable, consistent schemes, the global error <span class="math inline">\(E^n\)</span> can be expected to maintain the same trend as the local truncation error <span class="math inline">\(\varepsilon^n\)</span>, though this claim would require a formal proof. As an example, the output of scheme <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a> is compared against the exact solution given in <a href="#eq:SHOexact" data-reference-type="eqref" data-reference="eq:SHOexact">[eq:SHOexact]</a>. The initial conditions in the continuous system are set as <span class="math inline">\(x_0 =1\)</span>, <span class="math inline">\(v_0 = 0\)</span>, giving <span class="math inline">\(x(t) = \cos(\omega_0 t)\)</span>. The initial conditions in the discrete scheme are given as <span class="math inline">\(x^0 = x_0 = 1\)</span>, <span class="math inline">\(x^1 = \cos(\omega_0 k)\)</span> (which discretises the exact solution at the time <span class="math inline">\(t=k\)</span>). Fig. <a href="#fig:SHOerrs" data-reference-type="ref" data-reference="fig:SHOerrs">1.4</a> (a double log plot) shows that indeed slopes of 2 are recovered.</p>
<figure>
<embed src="Figures/OscError.pdf" id="fig:SHOerrs" /><figcaption aria-hidden="true">Global error of scheme <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a>. Here, <span class="math inline">\(nk = 1\)</span>. Initial conditions are given as <span class="math inline">\(x_0 = 1\)</span>, <span class="math inline">\(v_0 = 0\)</span>, giving <span class="math inline">\(x(t) = \cos(\omega_0 t)\)</span>. The numerical initial conditions are <span class="math inline">\(x^0 = 1\)</span>, <span class="math inline">\(x^1 = \cos(\omega_0 k)\)</span>. The three lines correspond to <span class="math inline">\(\omega_0 = 100\)</span> rad/s (solid), <span class="math inline">\(\omega_0 = 200\)</span> rad/s (dashed), <span class="math inline">\(\omega_0 = 300\)</span> rad/s (dash-dotted). Sample rate <span class="math inline">\(f_s = 2000\)</span> Hz.</figcaption>
</figure>
<p>The behaviour of the global error as a function of the time step <span class="math inline">\(k\)</span> encapsulates the idea of convergence. A scheme is convergent if <span class="math display">\[\label{eq:convDef}
    \lim_{k\rightarrow 0} (x(t_n)-x^n) = \lim_{k\rightarrow 0} E^n = 0.\]</span> In practice, as the time step is decreased, the global error goes to zero. We will come back to the idea of convergence later on. In general, a stable and consistent method is also convergent (this should be proven rigourously, and we will postpone this discussion until later sections).</p>
<h4 id="sec:Init">Initialisation</h4>
<p>In the previous subsection, the scheme was initialised exactly using knowledge coming from the exact solution <span class="math inline">\(x(t)\)</span> as per <a href="#eq:SHOexact" data-reference-type="eqref" data-reference="eq:SHOexact">[eq:SHOexact]</a>. Of course, generally an exact solution is not available, and schemes must be initialised in some other manner. Since we usually know the initial position and velocity of the oscillator (in continuous time) <span class="math inline">\(x_0, v_0\)</span>, we would like to know how to use this information to extract suitable initial values for the time series, i.e. <span class="math inline">\(x^0\)</span>, <span class="math inline">\(x^1\)</span>.</p>
<figure>
<embed src="Figures/OscErrorOrder.pdf" id="fig:SHOerrsOrders" /><figcaption aria-hidden="true">Global error of scheme <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a>. Initialisation with first-order accurate (markers) and second-order accurate (lines) initial conditions. Here, <span class="math inline">\(nk = 1\)</span>. Initial conditions are given as <span class="math inline">\(x_0 = 0.5\)</span>, <span class="math inline">\(v_0 = 0.5\)</span>. The numerical initial conditions are <span class="math inline">\(x^0 = x_0\)</span>, <span class="math inline">\(x^1 = kv_0 + x_0 - 0.5 k^2 \omega_0^2 x_0\)</span> (lines), <span class="math inline">\(x^0 = x_0\)</span>, <span class="math inline">\(x^1 = kv_0 + x_0\)</span> (markers). The three cases correspond to <span class="math inline">\(\omega_0 = 100\)</span> rad/s, <span class="math inline">\(\omega_0 = 200\)</span> rad/s, <span class="math inline">\(\omega_0 = 300\)</span> rad/s.</figcaption>
</figure>
<p>Obviously, one can set <span class="math display">\[x^0=x_0\]</span> For <span class="math inline">\(x^1\)</span>, one possible solution is to use a simple forward difference to compute it from <span class="math inline">\(v_0\)</span> and <span class="math inline">\(x^0\)</span>, i.e. <span class="math display">\[\delta_{t+}x^0 = v_0 \,\,\, \rightarrow \,\,\, x^1 = x^0 + kv_0.\]</span> This approximation to the initial conditions is only <em>first-order accurate.</em> This is easily proven via Taylor series arguments. One may be tempted then to use the centered difference <span class="math inline">\(\delta_{t\cdot}\)</span>, instead of the forward difference <span class="math inline">\(\delta_{t+}\)</span>, since it yields a second-order accurate approximation to the time derivative. Of course, this is not possible directly, since the stencil of <span class="math inline">\(\delta_{t\cdot}\)</span> is too large: we do not know what <span class="math inline">\(x^{-1}\)</span> is (in fact, this value is not defined). However, consider the following expression for the centered time difference: <span class="math display">\[\label{eq:dtdInit}
    \delta_{t\cdot}= \delta_{t+}- \frac{k}{2} \delta_{tt}= \frac{d}{dt} + O(k^2).\]</span> Of course, the value of <span class="math inline">\(\delta_{tt}\)</span> is not substituted directly, rather via <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a>, i.e. <span class="math inline">\(\delta_{tt}= -\omega_0^2\)</span>. Thus, a second-order accurate intial condition can be given as <span class="math display">\[\delta_{t\cdot}x^0 = v_0 \,\,\, \rightarrow \,\,\, x^1 = x^0 + kv_0 - \frac{k^2}{2}\omega_0^2x^0.\]</span> Fig. <a href="#fig:SHOerrsOrders" data-reference-type="ref" data-reference="fig:SHOerrsOrders">1.5</a> shows the error plots, computed against the exact solution <a href="#eq:SHOexact" data-reference-type="eqref" data-reference="eq:SHOexact">[eq:SHOexact]</a>, displaying the expected trends in the limit of high sample rate. Note that, for lower values of the sample rate, the error of the first-order accurate scheme may in fact be lower than the second-order accurate scheme, though in the limit of vanishing time step the correct trends are recovered, and convergence is of course faster for the second-order accurate schemes.</p>
<p>Higher order accurate approximations are of course possible. Here is a list, obtained using Taylor-series arguments. <span class="math display">\[\begin{aligned}
\label{eq:HigherOrderICsOscillator}
\delta_{t+}x^0 &amp;= v_0 \quad \text{first order}\\
\left(\delta_{t+}-\frac{k}{2}\delta_{tt}\right) x^0 &amp;= v_0 \quad \text{second order}\\
\left(\delta_{t+}-\frac{k}{2}\delta_{tt}- \frac{k^2}{6}\delta_{t+}\delta_{tt}\right) x^0 &amp;= v_0 \quad \text{third order}\\
\left(\delta_{t+}-\frac{k}{2}\delta_{tt}- \frac{k^2}{6}\delta_{t+}\delta_{tt}- \frac{k^3}{24}\delta_{tt}^2 \right) x^0 &amp;= v_0 \quad \text{fourth order}\end{aligned}\]</span> In the expressions above, substituting <span class="math inline">\((\delta_{tt})^p = (-\omega_0)^p\)</span> gives a way to compute <span class="math inline">\(x^1\)</span>, knowing <span class="math inline">\(x^0\)</span> and <span class="math inline">\(v_0\)</span>.</p>
<h3 id="eq:ModEqTechniques">Frequency warping and modified equation techniques</h3>
<p>The discussion about accuracy has so far dealt with the idea of order-accuracy, i.e. how the global error <span class="math inline">\(E^n\)</span> behaves as the time step <span class="math inline">\(k\)</span> is decreased. It was seen that finite difference scheme usually behave in such a way that <span class="math inline">\(|E^n| = O(k^p)\)</span>, where <span class="math inline">\(p \geq 1\)</span> is the order of accuracy. This is, of course, one way of looking at how well a scheme performs. For the oscillator, it may be useful to measure the degree of accuracy in the frequency domain. Solution <a href="#eq:sol_z_SHO" data-reference-type="eqref" data-reference="eq:sol_z_SHO">[eq:sol_z_SHO]</a> suggests that the solutions are oscillating with natural frequency <span class="math display">\[\label{eq:ErrFreqs}
    \omega = \frac{1}{k}\arctan{\frac{\omega_0^2k^2\sqrt{\frac{4}{\omega_0^2 k^2}-1}}{(2-\omega_0^2 k^2)}} = \omega_0 \left(1 + \frac{\omega_0^2 k^2}{24} +  \frac{3 \omega_0^4 k^4}{640} + O(\omega_0^6k^6) \right),\]</span> and hence the natural frequency computed by scheme <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a> is second-order accurate compared to the natural frequency of the continuous system. See also left panel of Fig. <a href="#eq:FreqWarping" data-reference-type="ref" data-reference="eq:FreqWarping">1.6</a>. The error in frequency can be quite audible, as <span class="math inline">\(\omega_0\)</span> approaches the limit of stability <span class="math inline">\(\omega_{max}=2/k\)</span>. In practice, the cent deviation from <a href="#eq:ErrFreqs" data-reference-type="eqref" data-reference="eq:ErrFreqs">[eq:ErrFreqs]</a> is given by <span class="math display">\[1200 \log_{2}\frac{\omega}{\omega_0} = 1200\log_2\left(1 + \frac{\omega_0^2 k^2}{24} +  \frac{3 \omega_0^4 k^4}{640} + O(\omega_0^6k^6) \right),\]</span> and this is shown in the right panel of Fig. <a href="#eq:FreqWarping" data-reference-type="ref" data-reference="eq:FreqWarping">1.6</a>.</p>
<figure>
<embed src="Figures/FreqWarp.pdf" id="eq:FreqWarping" /><figcaption aria-hidden="true">Frequency warping (left) and cent deviation (right) of scheme <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a>. In the left panel, three natural frequencies are shown: 100 rad/s (solid), 200 rad/s (dahsed), 300 rad/s (dash-dotted). The right panel shows cent deviation up to <span class="math inline">\(O(\omega_0^6k^6)\)</span> (solid), and exact (dashed).</figcaption>
</figure>
<p>The frequency warping effects are quite evident. It may be preferable, then, to construct schemes with a higher accuracy. This, of course, cannot be done by merely using difference operators with a larger stencil, such as the one given in <a href="#eq:FourthOrderdtt" data-reference-type="eqref" data-reference="eq:FourthOrderdtt">[eq:FourthOrderdtt]</a>: this would result in unstable behaviour! A different approach, known as <em>modified equation method</em>, can be constructed starting from Taylor-series arguments. One has <span class="math display">\[\label{eq:dttExpasion}
    \delta_{tt}= \sum_{l=1}^{\infty}\frac{2k^{2(l-1)}}{(2l)!}\frac{d^{2l}}{dt^{2l}}= \frac{d^2}{dt^2} + \frac{k^2}{12}\frac{d^4}{dt^4} + \frac{k^4}{360}\frac{d^6}{dt^6} + O(k^6).\]</span> Considering for the moment the expansion up to the term <span class="math inline">\(l=2\)</span>, it is natural to add a term <span class="math inline">\(-\frac{k^2}{12}\delta_{tt}\delta_{tt}x^n\)</span> on the left-hand side of <a href="#eq:Scheme1" data-reference-type="eqref" data-reference="eq:Scheme1">[eq:Scheme1]</a>, in order to cancel the <span class="math inline">\(O(k^2)\)</span> error. Then, one may use <span class="math inline">\(\delta_{tt}= -\omega_0^2\)</span> twice, to get <span class="math display">\[\delta_{tt}x^n = \frac{1}{k^2}\left(-\omega_0^2k^2 + \frac{\omega_0^4 k^4}{12} \right) x^n,\]</span> and of course the local truncation error <span class="math inline">\(\varepsilon^n\)</span> is now <span class="math inline">\(O(k^4)\)</span>. Considering now the next term in the series, proportional to <span class="math inline">\(k^4\)</span>, and using <span class="math inline">\(\delta_{tt}= -\omega_0^2\)</span> three times, one gets <span class="math display">\[\delta_{tt}x^n = \frac{1}{k^2}\left(-\omega_0^2k^2 + \frac{\omega_0^4 k^4}{12} - \frac{\omega_0^6k^6}{320}\right) x^n,\]</span> and this approximation is <span class="math inline">\(O(k^6)\)</span>. Of course, one may go on and add more and more terms to the series. The expansion can be rewritten conveniently as <span class="math display">\[\frac{1}{k^2}\left(-\omega_0^2k^2 + \frac{\omega_0^4 k^4}{12} - \frac{\omega_0^6 k^6}{320} + ...\right) = \frac{2}{k^2}\left(-1 + \underbrace{1 - \frac{\omega_0^2k^2}{2} + \frac{\omega_0^4k^4}{4!} - \frac{\omega_0^6k^6}{6!} + ...}_{\cos(\omega_0k)} \right),\]</span> and thus, adding infinite terms to the series results in <span class="math display">\[\label{eq:SHOexactNum}
    \delta_{tt}x^n =  \frac{2}{k^2}\left(-1 + \cos(\omega_0 k) \right)x^n.\]</span> Since the series expansion converges to a known function in this case, we can say that scheme <a href="#eq:SHOexactNum" data-reference-type="eqref" data-reference="eq:SHOexactNum">[eq:SHOexactNum]</a> solves <a href="#eq:SHOreal" data-reference-type="eqref" data-reference="eq:SHOreal">[eq:SHOreal]</a> <em>exactly</em>. Of course, this is somewhat too strong a statement: following the discussion in Sec. <a href="#sec:Init" data-reference-type="ref" data-reference="sec:Init">1.1.1.4</a>, we know that the scheme is only going to be as accurate as its initial conditions in this case. However, scheme <a href="#eq:SHOexactNum" data-reference-type="eqref" data-reference="eq:SHOexactNum">[eq:SHOexactNum]</a> does not introduce errors in the frequency domain.</p>
<h2 id="sec:LossSHO">Loss</h2>
<figure>
<img src="Figures/PhaseSpLoss.png" id="fig:dampedOsc" alt="Time evolution and phase portraits of lightly damped oscillator ((a),(b)), and overdamped oscillator ((c),(d)). The natural frequency is \omega_0=100 rad/s, and the decay times are \tau_{60}=5 s (lightly damped), 0.05 s (overdamped). Initial conditions are x_0=-0.01 m, v_0=0.04 m/s." /><figcaption aria-hidden="true">Time evolution and phase portraits of lightly damped oscillator ((a),(b)), and overdamped oscillator ((c),(d)). The natural frequency is <span class="math inline">\(\omega_0=100\)</span> rad/s, and the decay times are <span class="math inline">\(\tau_{60}=5\)</span> s (lightly damped), 0.05 s (overdamped). Initial conditions are <span class="math inline">\(x_0=-0.01\)</span> m, <span class="math inline">\(v_0=0.04\)</span> m/s.</figcaption>
</figure>
<p>We are now going to study the behaviour of the oscillator in the presence of dissipative forces. These are always present in some form in real systems. Terms proportional to the velocity are usually a good starting point to model viscous damping. For the oscillator, a modification of <a href="#eq:SHOreal" data-reference-type="eqref" data-reference="eq:SHOreal">[eq:SHOreal]</a> can then be given as <span class="math display">\[\label{eq:SHOLoss}
    \frac{d^2 x}{dt^2} = -\omega_0^2 x - 2c \frac{dx}{dt}.\]</span> Here, <span class="math inline">\(c \geq 0\)</span> is a loss parameter (assumed constant, and measured in s<span class="math inline">\(^{-1}\)</span>). We remark that <a href="#eq:SHOLoss" data-reference-type="eqref" data-reference="eq:SHOLoss">[eq:SHOLoss]</a> is still a linear, time invariant system, and we may infer its stability properties via <em>ansatz</em> <a href="#eq:ansatz" data-reference-type="eqref" data-reference="eq:ansatz">[eq:ansatz]</a>. Thus, <span class="math display">\[\label{eq:LossAnsatz}
    \hat x e^{st}\left(s^2 + 2c s + \omega_0^2 \right) = 0, \,\, \text{implying}\,\, s_{\pm} = -{c} \pm \sqrt{{c}^2 - \omega_0^2}.\]</span> The qualitative behaviour of the oscillator will depend on the value of <span class="math inline">\(c\)</span> compared to <span class="math inline">\(\omega_0\)</span>, i.e. whether the square root in the expression for <span class="math inline">\(s_\pm\)</span> is real or pure imaginary. Two cases of interest may be extracted as follows:</p>
<ol type="1">
<li><p><span class="math inline">\(c&lt;\omega_0\)</span>. In this case, the oscillator is only lightly damped, and <span class="math inline">\(s_\pm = -c \pm j \sqrt{\omega_0^2-c^2}\)</span>. Remembering the definition of <span class="math inline">\(s\)</span> in <a href="#eq:LapT" data-reference-type="eqref" data-reference="eq:LapT">[eq:LapT]</a>, one may extract <span class="math inline">\(\sigma = - c\)</span>, <span class="math inline">\(\omega = \sqrt{\omega_0^2-c^2}\)</span>. The solution to <a href="#eq:SHOLoss" data-reference-type="eqref" data-reference="eq:SHOLoss">[eq:SHOLoss]</a> may then be written as <span class="math inline">\(x(t) = A_+ e^{s_+ t} + A_- e^{s_- t} = e^{-c t}\left(A_+ e^{j \omega t} + A_- e^{-j\omega t} \right)\)</span>. As per usual, the complex constants <span class="math inline">\(A_+,A_-\)</span> are determined from the intial conditions. The solution is in this case the product of an oscillating solution, times an exponentially damped envelope. The frequency of vibration is <span class="math inline">\(\omega = \omega_0\sqrt{1 - c^2/\omega_0^2}\approx \omega_0\left(1 - \frac{c^2}{2\omega_0^2} \right)\)</span> and is thus lower than the natural frequency of the undamped oscillator. In this case, motion is not strictly periodic, since the mass is never really going to extend as far at each oscillation because of the energy given away to losses. However, it still makes sense to speak of period of vibration, as <span class="math inline">\(\tau = 2\pi / \omega.\)</span></p></li>
<li><p><span class="math inline">\(c&gt;\omega_0\)</span>. In this case, <span class="math inline">\(s_\pm\)</span> are both real, and negative, i.e. <span class="math inline">\(s_\pm &lt; 0\)</span>. Thus, the solutions are still “physical” in that they die out exponentially as time increases. However, the mass does not oscillate. This case is sometimes referred to as <em>overdamped oscillator.</em></p></li>
</ol>
<p>Fig. <a href="#fig:dampedOsc" data-reference-type="ref" data-reference="fig:dampedOsc">1.7</a> shows illustrative examples of such cases. Note that, in phase space, the orbits spiral toward the centre, and motion is strictly speaking not periodic. A third case (<em>critically damped</em>) is obtained whenever <span class="math inline">\(c=\omega_0\)</span>. This case serves as a mathematical “boundary” between the overdamped and lightly damped cases, and is never really realised in practice. A useful quantity to quantify loss is the <em>decay time</em> <span class="math inline">\(\tau_{60}\)</span>. This is defined as the time taken by the oscillator to reduce its amplitude of vibration by 60 dB. In the lightly damped case, the amplitude envelope is simply <span class="math inline">\(e^{-ct}\)</span>. Thus, the implicit definition of <span class="math inline">\(\tau_{60}\)</span> is obtained as <span class="math display">\[\label{eq:tau60}
    -60 = 20 \log_{10}e^{-c\tau_{60}}, \,\, \text{and upon inversion: } \,\, \tau_{60} = \frac{3}{c} \ln(10).\]</span> This shows that the loss constant <span class="math inline">\(c\)</span> is most easily interpreted as a function of the decay time.</p>
<h3 id="energy-analysis">Energy analysis</h3>
<p>Qualitative results on stability can be obtained via energy analysis. Multiplying <a href="#eq:SHOLoss" data-reference-type="eqref" data-reference="eq:SHOLoss">[eq:SHOLoss]</a> by <span class="math inline">\(m \frac{dx}{dt}\)</span> and using the same indentities as for <a href="#eq:En1" data-reference-type="eqref" data-reference="eq:En1">[eq:En1]</a>, we get <span class="math display">\[\label{eq:EnBalLoss}
    \frac{d}{dt}\left( \frac{m}{2} \left(\frac{dx}{dt}\right)^2 + \frac{K x^2}{2}   \right) = -Q(t) \triangleq - 2mc \left( \frac{dx}{dt} \right)^2 \leq 0,\]</span> implying that the total energy is not increasing. Here, <span class="math inline">\(Q(t) \geq 0\)</span> is the power dissipated by the oscillator. Thus, bounds <a href="#eq:bnds" data-reference-type="eqref" data-reference="eq:bnds">[eq:bnds]</a> hold here as well. It is somewhat harder to draw more quantitative results here, without knowledge on the form of <span class="math inline">\(x(t).\)</span> However, it is easy to draw trajectories in phase space: instead of a closed loop, the mass now spirals toward the centre as a result of losses.</p>
<h3 id="a-finite-difference-scheme-1">A finite difference scheme</h3>
<p>As a discretisation for <a href="#eq:SHOLoss" data-reference-type="eqref" data-reference="eq:SHOLoss">[eq:SHOLoss]</a> is obtained as <span class="math display">\[\label{eq:FDSHOLoss}
    \delta_{tt}x^n = -\omega_0^2 x^n -2c \delta_{t\cdot}x^n,\]</span> yielding an update equation <span class="math display">\[\left(ck+1\right) x^{n+1} = (2-\omega_0^2 k^2) x^n + (ck - 1)x^n.\]</span> Using the definition of local truncation error <span class="math inline">\(\varepsilon^n\)</span>, as per <a href="#eq:LTEdef" data-reference-type="eqref" data-reference="eq:LTEdef">[eq:LTEdef]</a>, one gets <span class="math inline">\(\varepsilon^n = O(k^2)\)</span>, <span class="math inline">\(\forall n\)</span>, showing that the LTE is second-order accurate. Stability may be inferred using either frequency-domain analysis, or energy methods. Application of the former via the ansatz <span class="math inline">\(x^n = \hat x z^n\)</span> gives <span class="math display">\[\label{eq:SolFdLoss}
    \hat x z^n \left((1+ck)z - (2-\omega_0^2 k^2) -(ck-1)z^{-1}\right), \,\, z_\pm = \frac{2-\omega_0^2k^2 \pm \sqrt{(2-\omega_0^2k^2)^2 - 4(1-ck)(1+ck)}}{2(1+ck)},\]</span> and, using the Schur-Cohn condition <a href="#eq:SchurCohnStab" data-reference-type="eqref" data-reference="eq:SchurCohnStab">[eq:SchurCohnStab]</a>, <span class="math inline">\(|z_\pm|&lt;1\)</span> if and only if <span class="math display">\[k &lt; \frac{2}{\omega_0},\]</span> that is the same as <a href="#eq:StabCondSHO" data-reference-type="eqref" data-reference="eq:StabCondSHO">[eq:StabCondSHO]</a>. The same condition may be arrived at via energy analysis. To that end, multiply <a href="#eq:FDSHOLoss" data-reference-type="eqref" data-reference="eq:FDSHOLoss">[eq:FDSHOLoss]</a> by <span class="math inline">\(\delta_{t\cdot}x^n\)</span>, <span class="math display">\[\delta_{t\cdot}x^n \, \delta_{tt}x^n = - \delta_{t\cdot}x^n \, \omega_0^2 x^n - 2c (\delta_{t\cdot}x^n)^2.\]</span> Using identities <a href="#eq:IdsFD" data-reference-type="eqref" data-reference="eq:IdsFD">[eq:IdsFD]</a>, and multiplying by the mass <span class="math inline">\(m\)</span> to restore units of energy, one gets <span class="math display">\[\delta_{t+}\left( \frac{m}{2}{(\delta_{t-}x^n)^2} + \frac{K}{2} {x^n e_{t-}x^n} \right) = - 2mc (\delta_{t\cdot}x^n)^2 \leq 0,\]</span> which is a discrete counterpart of <a href="#eq:EnBalLoss" data-reference-type="eqref" data-reference="eq:EnBalLoss">[eq:EnBalLoss]</a>. Thus, the discrete energy is non-increasing, and when the total energy is itself non-negative, boundedness of the solution results. Thus, the stability condition <a href="#eq:StabCondSHO" data-reference-type="eqref" data-reference="eq:StabCondSHO">[eq:StabCondSHO]</a> is recovered in this case as well. Of course, <a href="#eq:SHObound" data-reference-type="eqref" data-reference="eq:SHObound">[eq:SHObound]</a> holds in this case too. The numerical decay time may be established via knowledge of the solutions <span class="math inline">\(z_\pm\)</span> in <a href="#eq:SolFdLoss" data-reference-type="eqref" data-reference="eq:SolFdLoss">[eq:SolFdLoss]</a>. Assuming oscillating behaviour, <span class="math inline">\(z_\pm\)</span> are complex conjugates with absolute value <span class="math display">\[|z_\pm| = \sqrt{\frac{1-ck}{1+ck}}.\]</span> Thus, the numerical decay time index <span class="math inline">\(n_{60}\)</span> is given by <span class="math display">\[-60 = 20\log_{10}\left({\frac{1-ck}{1+ck}}\right)^{n_{60}/2},\,\,\,\, n_{60}k = \frac{6k \ln(10)}{\ln \frac{1+ck}{1-ck}}\approx \tau_{60} - c k^2 \ln(10),\]</span> showing that the numerical decay time (in seconds) is <span class="math inline">\(O(k^2)\)</span> compared to the exact decay time <span class="math inline">\(\tau_{60}\)</span> defined in <a href="#eq:tau60" data-reference-type="eqref" data-reference="eq:tau60">[eq:tau60]</a>.</p>
<h3 id="higher-order-schemes">Higher-order schemes</h3>
<p>Higher-order accurate schemes may of course be obtained in this case as well. To that end, consider the definition of the LTE, as <span class="math display">\[\left(\delta_{tt}+2c\delta_{t\cdot}\right) x(t_n) = - \omega_0^2 x(t_n) + \varepsilon^n,\]</span> where <span class="math inline">\(x(t_n)\)</span> is the true solution. Expanding in a Taylor series, one has <span class="math display">\[\left(\frac{d^2}{dt^2} + 2c \frac{d}{dt} \right)\left(1 + \frac{k^2}{6}\frac{d^2}{dt^2}\right)x(t_n) - \frac{k^2}{12}\frac{d^4}{dt^4}x(t_n) + O(k^4) = - \omega_0^2 x(t_n) + \varepsilon^n.\]</span> This suggests the use the following modified scheme, in order to cancel the terms proportional to <span class="math inline">\(k^2\)</span>: <span class="math display">\[\left(\delta_{tt}+ 2c \delta_{t\cdot}\right)\left(1 - \frac{k^2}{6}\delta_{tt}\right)x^n + \frac{k^2}{12}\delta_{tt}\delta_{tt}x^n = -\omega_0^2 x^n.\]</span> Since <span class="math inline">\(\delta_{tt}+ 2c \delta_{t\cdot}= -\omega_0^2 + O(k^2)\)</span>, the scheme above can be written as (to the order <span class="math inline">\(O(k^4)\)</span>): <span class="math display">\[\left(\delta_{tt}+ 2c \delta_{t\cdot}\right)x^n - \frac{k^2}{6}(-\omega_0^2) \delta_{tt}x^n + \frac{k^2}{12}\delta_{tt}\delta_{tt}x^n = -\omega_0^2 x^n.\]</span> The hard bit left is to find a suitable approximation to <span class="math inline">\(\delta_{tt}\delta_{tt}\)</span>, involving at most a stencil of width 2. This can be accomplised in the following way <span class="math display">\[\label{eq:dttsq}
    \delta_{tt}\delta_{tt}\approx \left(-\omega_0^2 e_{t-}-2c \delta_{t-}\right)\left(-\omega_0^2 e_{t+}-2c \delta_{t+}\right) = \omega_0^4 + 4c \omega_0^2 \delta_{t\cdot}+ 4c^2 \delta_{tt}.\]</span> Putting it all together, one obtains a fourth-order accurate approximation to <a href="#eq:SHOLoss" data-reference-type="eqref" data-reference="eq:SHOLoss">[eq:SHOLoss]</a> as <span class="math display">\[\left(1 + \frac{k^2}{6}(\omega_0^2 + 2c^2) \right)\delta_{tt}x^n = -\omega_0^2\left(1 + \frac{\omega_0^2 k^2}{12} \right) x^n - 2c \left(1 + \frac{\omega_0^2k^2}{6} \right)\delta_{t\cdot}x^n.\]</span> Higher-order accurate schemes can may be obtained this way, i.e. finding approximations to <span class="math inline">\(\delta_{tt}^p\)</span>, involving only operators of width 2. A sketch of the idea is given briefly here. From <a href="#eq:dttsq" data-reference-type="eqref" data-reference="eq:dttsq">[eq:dttsq]</a>, one may construct <span class="math inline">\(\delta_{tt}^3\)</span> in the following way: <span class="math display">\[\begin{aligned}
    \delta_{tt}\delta_{tt}\delta_{tt}\approx \left( \omega_0^4 + 4c \omega_0^2 \delta_{t\cdot}+ 4c^2 \delta_{tt}\right)\delta_{tt}\approx                                              
    \left( \omega_0^4e_{t-}+ 4c \omega_0^2 \delta_{t-}+ 4c^2 (-\omega_0^2e_{t-}-2c\delta_{t-})\right)\delta_{tt}\approx                                          \\\left( \omega_0^4e_{t-}+ 4c \omega_0^2 \delta_{t-}+ 4c^2 (-\omega_0^2e_{t-}-2c\delta_{t-})\right)(-\omega_0^2e_{t+}-2c\delta_{t+})= \\
    (-\omega_0^6 + 4c^2 \omega_0^4) + \left(-6 c \omega_0^4 + 16 c^3 \omega_0^2 \right)\delta_{t\cdot}+ \left( -8 c^2 \omega_0^2 + 16 c^4\right)\delta_{tt}, \end{aligned}\]</span> showing that <span class="math inline">\(\delta_{tt}^3\)</span> can be approximated using a stencil of width 2. One may of course use the modified equation technique described above to any desired order. Luckily, the oscillator with loss also posseses an exact solution, where “exact” is intended in the same way as for <a href="#eq:SHOexact" data-reference-type="eqref" data-reference="eq:SHOexact">[eq:SHOexact]</a> (i.e. exact up to the accuracy order of the intial conditions). Considering again the continuous equation with loss, <a href="#eq:SHOLoss" data-reference-type="eqref" data-reference="eq:SHOLoss">[eq:SHOLoss]</a>, under the following transformation <span class="math display">\[X(t) = e^{ct} x(t),\]</span> one gets <span class="math display">\[\frac{d^2X}{dt^2} + \left(\omega_0^2-c^2 \right) X = 0.\]</span> Thus, the exact scheme <a href="#eq:SHOexactNum" data-reference-type="eqref" data-reference="eq:SHOexactNum">[eq:SHOexactNum]</a> for the undamped oscillator can be applied to the transformed variable <span class="math inline">\(X\)</span>. When transformed back to <span class="math inline">\(x\)</span>, this gives <span class="math display">\[\label{eq:SHO2}
    % \left(\dtt + \frac{2e^{-c k}}{k^2} \left(e^{c k}-\cos\left(\sqrt{\omega_0^2-c^2} k\right)\right) + \frac{e_{t-}}{k^2} (e^{-2c k}-1)\right)x^n = 0.
    \delta_{tt}x^n = \left(-\frac{2}{k^2}\left(1 - \cos\left((\sqrt{\omega_0^2-c^2} \, k\right) \right)- \frac{e_{t+}(e^{ck}-1) + e_{t-}(e^{-ck}-1)}{k^2}\right)x^n\]</span> This scheme solves <a href="#eq:SHOLoss" data-reference-type="eqref" data-reference="eq:SHOLoss">[eq:SHOLoss]</a> exactly, in particular, the frequency of oscillation and the numerical decay time are exact.</p>
<h2 id="forced-oscillations">Forced Oscillations</h2>
<p>The equation of the oscillator including loss and source terms is given as <span class="math display">\[\label{eq:SHOForced}
    \frac{d^2 x}{dt^2} = -\omega_0^2 x - 2c \frac{dx}{dt} + f(t),\]</span> where <span class="math inline">\(f(t)\)</span> is a time-dependent force per unit mass. Energy analysis leads here to the following energy balance <span class="math display">\[\frac{d}{dt}\left( \frac{m}{2} \left(\frac{dx}{dt}\right)^2 + \frac{K x^2}{2}   \right) = - Q(t) + P(t),\]</span> where <span class="math inline">\(Q(t) = 2mc \left(\frac{dx}{dt}\right)^2\)</span> is the dissipated power, and where <span class="math inline">\(P(t) = m\frac{dx}{dt}f(t)\)</span> is the injected power.</p>
<p>In this case, the system is still linear, but it is not time invariant. Solutions via transform techniques can be obtained, involving the use of the one-sided Laplace transform <a href="#eq:LapT" data-reference-type="eqref" data-reference="eq:LapT">[eq:LapT]</a> so to incorporate the effects of the initial conditions and of the external forcing. In this case, substitution of the simpler <em>ansatz</em> <a href="#eq:ansatz" data-reference-type="eqref" data-reference="eq:ansatz">[eq:ansatz]</a> is not possible, because of the presence of the forcing term. Considering <span class="math inline">\(t\geq 0\)</span>, the application of the one-sided Laplace transform in <a href="#eq:SHOForced" data-reference-type="eqref" data-reference="eq:SHOForced">[eq:SHOForced]</a> gives <span class="math display">\[\int_0^{\infty} \left( \frac{d^2 x}{dt^2} + \omega_0^2 x + 2c \frac{dx}{dt} - f(t)\right)e^{-st} \, \mathop{}\!\mathrm{d}t = 0.\]</span> Using integration by parts, one gets (remember that we defined <span class="math inline">\(x_0 = x(0), v_0 = dx(0)/dt\)</span>): <span class="math display">\[\hat x(s)\left( s^2 + 2cs + \omega_0^2\right) = (s+2c)x_0 + v_0 + \hat{f}(s),\]</span> and thus <span class="math display">\[\hat x(s) = \frac{(s+2c)x_0 + v_0 + \hat{f}(s)}{s^2 + 2cs + \omega_0^2}.\]</span> The expression above may be decomposed into the <em>transient</em> and <em>forced response</em>. The transient is given by the contribution of the initial conditions only, without external forcing, so: <span class="math display">\[\hat x(s) = \hat x_{tr}(s) + \hat x_{fr}(s) = \frac{(s+c)x_0 + (v_0 + cx_0)}{s^2 + 2cs + \omega_0^2} + \frac{\hat{f}(s)}{s^2 + 2cs + \omega_0^2}.\]</span> This shows that the contributions of the transient and of the forced response are independent of each other: they add up in the final response. The solution in the time domain is obtained upon inversion of <span class="math inline">\(\hat x(s).\)</span> It is best to write the denominator of <span class="math inline">\(\hat x(s)\)</span> as <span class="math inline">\((s+c)^2 + \left(\sqrt{\omega_0^2-c^2}\right)^2\)</span>, since this is the form reported in <a href="#eq:LaplTtable" data-reference-type="eqref" data-reference="eq:LaplTtable">[eq:LaplTtable]</a>. Using these (with <span class="math inline">\(a = \sqrt{\omega_0^2-c^2}\)</span>), the solution to the transient is <span class="math display">\[x_{tr}(t) = e^{-ct}\left(x_0 \cos\left(\sqrt{\omega_0^2-c^2}\,\,t\right) + \frac{v_0 + c x_0}{\sqrt{\omega_0^2-c^2}}\sin\left(\sqrt{\omega_0^2-c^2}\,\,t\right)\right),\]</span> which is of course the same as <a href="#eq:LossAnsatz" data-reference-type="eqref" data-reference="eq:LossAnsatz">[eq:LossAnsatz]</a> (we did not go through the substitution of the initial conditions there, but the result is the same). For the forced response, we may employ the <em>convolution</em> property of the Laplace transform, which states that <span class="math display">\[\mathcal{L}^{-1}(\hat a(s)\hat b(s)) = \int_{0}^t a(t-u)b(u) \, \mathop{}\!\mathrm{d}u,\]</span> and thus, using <span class="math inline">\(\hat a = 1/((s+c)^2 + (\sqrt{\omega_0^2-c^2})^2)\)</span>, <span class="math inline">\(\hat b = \hat f\)</span>, one gets <span class="math display">\[\label{eq:steadyStSHO}
    x_{fr}(t) = \int_0^t \frac{e^{-c(t-u)}}{\sqrt{\omega_0^2-c^2}}\sin\left(\sqrt{\omega_0^2-c^2}\,\,(t-u)\right) f(u) \, \mathop{}\!\mathrm{d}u.\]</span> This integral is not generally computable analytically. However, it also encapsulates the idea that the forced response to <em>any</em> forcing can be obtained as the convolution with the <em>impulse response</em>. To that end, considering <span class="math inline">\(f(t)=\delta(t-t_0)\)</span>, with delta being Dirac’s delta here, and denoting as per usual the initial time by <span class="math inline">\(t_0\)</span>, one gets from <a href="#eq:steadyStSHO" data-reference-type="eqref" data-reference="eq:steadyStSHO">[eq:steadyStSHO]</a>: <span class="math display">\[\label{eq:GreenSHO}
    G(t|t_0) = \frac{e^{-c(t-t_0)}}{\sqrt{\omega_0^2-c^2}}\sin\left(\sqrt{\omega_0^2-c^2}\,\,(t-t_0)\right), \,\,\,\, t\geq t_0,\]</span> and <span class="math inline">\(G\)</span> is zero for <span class="math inline">\(t&lt;t_0\)</span>. The symbol <span class="math inline">\(G(t|t_0)\)</span> denotes the <em>Green’s function</em> of the harmonic oscillator, i.e. the response to a Dirac impulse at <span class="math inline">\(t=t_0\)</span>. In particular, for zero initial conditions, the forced response is also the total response of the system.</p>
<figure>
<img src="Figures/ForcedOsc.png" alt="Time evolution of forced oscillator. Dashed-dotted line is response to initial conditions, dahsed line is Green’s function [eq:GreenSHO], solid line is total response. The oscillator is activated with a Dirac impulse at t_0=0. Initial conditions are x_0=-0.01 m, v_0=0.04 m/s. The natural frequency of the oscillator is \omega_0 = 100 rad/s, and the decay time is \tau_{60} = 5 s." /><figcaption aria-hidden="true">Time evolution of forced oscillator. Dashed-dotted line is response to initial conditions, dahsed line is Green’s function <a href="#eq:GreenSHO" data-reference-type="eqref" data-reference="eq:GreenSHO">[eq:GreenSHO]</a>, solid line is total response. The oscillator is activated with a Dirac impulse at <span class="math inline">\(t_0=0\)</span>. Initial conditions are <span class="math inline">\(x_0=-0.01\)</span> m, <span class="math inline">\(v_0=0.04\)</span> m/s. The natural frequency of the oscillator is <span class="math inline">\(\omega_0 = 100\)</span> rad/s, and the decay time is <span class="math inline">\(\tau_{60} = 5\)</span> s.</figcaption>
</figure>
<p>The stability of system <a href="#eq:SHOForced" data-reference-type="eqref" data-reference="eq:SHOForced">[eq:SHOForced]</a> may be adapted to include the effects of external forcing. We are not going to bother as much here, and we will assume that the solution will not “blow up” in a finite time if the source remains bounded. In particular, if the source is itself bounded, we remark that integral <a href="#eq:steadyStSHO" data-reference-type="eqref" data-reference="eq:steadyStSHO">[eq:steadyStSHO]</a> remains bounded.</p>
<h3 id="response-to-harmonic-input-forcing">Response to harmonic input forcing</h3>
<p>Consider now a harmonic input of the form <span class="math display">\[\label{eq:harmoForceLinear}
    f(t) = F e^{j\omega t},\]</span> with <span class="math inline">\(F \in \mathbb{C}\)</span> being a complex forcing amplitude, and <span class="math inline">\(\omega \in \mathbb{R}^+_0\)</span> being the input forcing radian frequency (not to be confused with the natural frequency of the oscillator). We are only going to assume that the oscillator will eventually fall into a steady-state here, since from the previous discussion we know that the transient response will die out after sufficient time has elapsed, and since the forcing is of harmonic type. According to <a href="#eq:steadyStSHO" data-reference-type="eqref" data-reference="eq:steadyStSHO">[eq:steadyStSHO]</a>, one may compute the steady-state via the convolution integral. However, in this case one may equivalently assume that the steady state vibrates at the frequency of the input, thus <span class="math display">\[x(t) = X e^{j\omega t},\]</span> where we removed the index <span class="math inline">\(st\)</span> since we are now assuming that the transient has completely died out, and thus we can identify the whole solution <span class="math inline">\(x(t)\)</span> of <a href="#eq:SHOForced" data-reference-type="eqref" data-reference="eq:SHOForced">[eq:SHOForced]</a> with the steady-state. <span class="math inline">\(X\)</span> is here a complex amplitude. Substituting into the equation of motion results in <span class="math display">\[\label{eq:TransXF}
    \frac{X}{F} = \frac{1}{\omega_0^2-\omega^2+2jc\omega} =  \frac{\omega_0^2-\omega^2-2jc\omega}{(\omega_0^2-\omega^2)^2+4c^2\omega^2}.\]</span> Since <span class="math inline">\(X,F\)</span> are complex constants, the tangent of the phase angle is obtained as the ratio between the imaginary and real parts, i.e. <span class="math display">\[\tan \left(\angle \frac{X}{F}\right) = -\frac{2c\omega}{\omega_0^2 - \omega^2}\]</span> One should pay attention to the sign of the denominator when inverting the tangent function. Hence, the phase starts out at <span class="math inline">\(0\)</span> when <span class="math inline">\(\omega \approx 0\)</span>; then it reaches <span class="math inline">\(-\pi/2\)</span> when <span class="math inline">\(\omega \approx \omega_0\)</span>, and then approaches <span class="math inline">\(-\pi\)</span> as <span class="math inline">\(\omega \rightarrow \infty\)</span>. The absolute value of the transfer function is obtained as <span class="math display">\[\label{eq:XFlinearOsc}
    \left|\frac{X}{F}\right|  = \left((\omega_0^2-\omega^2)^2+4c^2\omega^2\right)^{-1/2},\]</span> which has a maximum at <span class="math inline">\(\omega = \omega_0 \left(1-2(c/\omega_0)^2 \right)^{1/2}\)</span>. The maximum is <span class="math display">\[\left|\frac{X}{F}\right|(\omega = \omega_0 \left(1-2(c/\omega_0)^2 \right)^{1/2}) \approx \frac{1}{2c\omega_0},\]</span> where factors of the order <span class="math inline">\(O(c^4)\)</span> were disregarded. The <em>bandwidth</em> of the transfer function is defined as the interval in frequency occurring between the frequencies <span class="math inline">\(\omega_+,\omega_-\)</span> that are found at <span class="math inline">\(\frac{1}{\sqrt{2}}\)</span> the maximum (these are the <em>half-power points</em>, since power is the square of the absolute value). To obtain <span class="math inline">\(\omega_\pm\)</span>, one simply uses this defintion, hence: <span class="math display">\[\left((\omega_0^2-\omega^2)^2+4c^2\omega^2\right)^{-1/2} = \left(2\sqrt{2} c\omega_0\right)^{-1}.\]</span> Solving for <span class="math inline">\(\omega\)</span>, and disregarding small terms, one gets <span class="math display">\[\omega_\pm \approx \omega_0\left(1 \pm {c} \right), \quad \rightarrow (\omega_+ - \omega_-) \approx 2c.\]</span> This shows that, for small damping values, one may recover the value of the decay time from the frequency response, after measuring the bandwith of the peak in the transfer function. As a cautionary note, the transfer function <span class="math inline">\(X/F\)</span> was obtained here in the case of sinusoidal input forcing, by “scanning” the frequency axis. The same transfer function may however be obtained via the impulse response <a href="#eq:GreenSHO" data-reference-type="eqref" data-reference="eq:GreenSHO">[eq:GreenSHO]</a>. To show this, it is sufficient to compute a Fourier transform and, again, we may resort to tables for this. Considering the transforms <a href="#eq:FouTtable" data-reference-type="eqref" data-reference="eq:FouTtable">[eq:FouTtable]</a> (with <span class="math inline">\(a = \sqrt{\omega_0^2 - c^2}\)</span>, <span class="math inline">\(x(t) = e^{-ct}\)</span>), one gets for the impulse response <a href="#eq:GreenSHO" data-reference-type="eqref" data-reference="eq:GreenSHO">[eq:GreenSHO]</a> <span class="math display">\[\mathcal{F}\left\{G(t|t_0=0)\right\}(\omega) = \frac{1}{\sqrt{2\pi}}\frac{1}{(\omega_0^2-\omega^2)+2jc\omega} = \frac{1}{\sqrt{2\pi}}\frac{\omega_0^2-\omega^2-2jc\omega}{(\omega_0^2-\omega^2)^2+4c^2\omega^2},\]</span> that is the same as <a href="#eq:TransXF" data-reference-type="eqref" data-reference="eq:TransXF">[eq:TransXF]</a> (up to a constant factor). Thus, knowledge of the impulse response is equivalent to “scanning” the frequency axis one frequency at a time. This equivalence is often employed experimentally, where the impulse response is obtained via deconvolution of appropriate sine sweeps.</p>
<h4 id="mechanical-impedance">Mechanical impedance</h4>
<p>Though <a href="#eq:TransXF" data-reference-type="eqref" data-reference="eq:TransXF">[eq:TransXF]</a> expresses the general relationship between output displacement and input forcing, it may be preferable to obtain the transfer function between output velocity and input forcing. Thus, <span class="math display">\[\frac{dx(t)}{dt} = j\omega e^{j\omega t} \triangleq V e^{j\omega t}.\]</span> Using this in <a href="#eq:TransXF" data-reference-type="eqref" data-reference="eq:TransXF">[eq:TransXF]</a> results in <span class="math display">\[\frac{V}{F} = \frac{j\omega(\omega_0^2 - \omega^2)-2c\omega^2}{(\omega_0^2-\omega^2)^2+4c^2\omega^2}.\]</span> The ratio <span class="math inline">\(V/F\)</span> (often denoted <span class="math inline">\(Y\)</span>) is the <em>mechanical admittance.</em> The inverse <span class="math inline">\(F/V\)</span> (often denoted <span class="math inline">\(Z\)</span>) is the <em>mechanical impedance.</em> The reason why it may be preferable to work with the impedance (or the admittance), rather than <span class="math inline">\(X/F\)</span>, is that velocity and force and power-conjugated quantities: in an energy framework, knowledge of the impedance/admittance allows to describe mechanical systems in an energy-consistent manner, as we will see in due course. For the phase angle, one has <span class="math display">\[\tan \left(\angle \frac{V}{F}\right) = \frac{\omega_0^2 - \omega^2}{(-2c\omega)}.\]</span> Thus, in this case the phase angle starts out at <span class="math inline">\(\pi/2\)</span> when <span class="math inline">\(\omega \approx 0\)</span>, then it goes to zero when <span class="math inline">\(\omega \approx \omega_0\)</span>, and then it approaches <span class="math inline">\(-\pi/2\)</span> as <span class="math inline">\(\omega \rightarrow \infty\)</span>. The absolute value is given by <span class="math display">\[\left| \frac{V}{F} \right| = \omega \left((\omega_0^2-\omega^2)^2+4c^2\omega^2\right)^{-1/2}.\]</span> Differentiating with respect to <span class="math inline">\(\omega\)</span>, one gets a maximum at <span class="math inline">\(\omega = \omega_0\)</span>. The maximum is <span class="math display">\[\left|\frac{V}{F}\right|(\omega = \omega_0) = (2c)^{-1}.\]</span></p>
<figure>
<img src="Figures/ImpedanceSHO.png" id="fig:LinearTransFunctPlots" alt="Absolute values and phase angles of the transfer functions for the linear oscillator. The natural frequency of the oscillator is \omega_0 = 100 rad/s, the decay times are set as \tau_{60}=\left\{1.0,1.2,1.4,1.6,1.8,2.0\right\} s" /><figcaption aria-hidden="true">Absolute values and phase angles of the transfer functions for the linear oscillator. The natural frequency of the oscillator is <span class="math inline">\(\omega_0 = 100\)</span> rad/s, the decay times are set as <span class="math inline">\(\tau_{60}=\left\{1.0,1.2,1.4,1.6,1.8,2.0\right\}\)</span> s</figcaption>
</figure>
<h4 id="finite-difference-schemes">Finite difference schemes</h4>
<figure>
<embed src="Figures/SlopesForced.pdf" id="fig:ErrSlopesForced" /><figcaption aria-hidden="true">Error slopes of scheme <a href="#eq:shoFDForced" data-reference-type="eqref" data-reference="eq:shoFDForced">[eq:shoFDForced]</a>. The oscillator is activated with a Dirac delta at <span class="math inline">\(t_0=0\)</span>. Initial conditions are <span class="math inline">\(x_0=-0.01\)</span> m, <span class="math inline">\(v_0=0.04\)</span> m/s. The natural frequencies of the oscillator are selected as <span class="math inline">\(\omega_0 = 100\)</span> rad/s (solid), <span class="math inline">\(\omega_0 = 200\)</span> rad/s (dashed), <span class="math inline">\(\omega_0 = 300\)</span> rad/s (dash-dotted), and the decay time is <span class="math inline">\(\tau_{60} = 5\)</span> s.</figcaption>
</figure>
<p>Finite difference schemes may be obtained in this case by any discretisation of <span class="math inline">\(f(t)\)</span>, of the desired accuracy. For second-order accuracy, <span class="math inline">\(f^n = \left\{f(t_n),\mu_{t\cdot}f(t_n), \mu_{tt}f(t_n) \right\}\)</span>, are all valid discretisations. Hence, a suitable second-order accurate scheme is obtained as <span class="math display">\[\label{eq:shoFDForced}
    \delta_{tt}x^n = -\omega_0^2 x^n -2c\delta_{t\cdot}x^n + f^n\]</span> Initialisation should be performed in such a way that second-order accuracy is preserved. In this respect, one may set <span class="math inline">\(x^0 = x_0\)</span>. Then, <a href="#eq:dtdInit" data-reference-type="eqref" data-reference="eq:dtdInit">[eq:dtdInit]</a> is used again, to get <span class="math display">\[(\delta_{t+}- \frac{k}{2}\delta_{tt})x^0 = v_0.\]</span> One has <span class="math display">\[\delta_{tt}x^0 \approx -\omega_0^2 x_0 - 2c \delta_{t+}x^0 + f^0.\]</span> Using these, one can extract <span class="math inline">\(x^1\)</span> as <span class="math display">\[x^1 = x^0 + \frac{k v_0 + \frac{k^2}{2}\left(-\omega_0^2 x^0 + f^0 \right)}{1+kc}\]</span> When approximating a Dirac delta at <span class="math inline">\(t_0=0\)</span>, one should use <span class="math inline">\(f^0 = 2/k\)</span>. Error slopes for scheme <a href="#eq:shoFDForced" data-reference-type="eqref" data-reference="eq:shoFDForced">[eq:shoFDForced]</a> are shown in Fig. <a href="#fig:ErrSlopesForced" data-reference-type="ref" data-reference="fig:ErrSlopesForced">1.9</a>.</p>
<script>
    window.MathJax = {
        loader: {load: ['[tex]/newcommand']},
        tex: {
            tags: 'ams',
            packages: {'[+]': ['newcommand']}
        }
    };
</script>
</body>
</html>
